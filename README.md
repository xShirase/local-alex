# MCP Agent Server â€“ Self-Hosted AI Assistant

This project is a privacy-first, self-hosted agent platform built on a modular architecture. It orchestrates local and remote LLMs, memory stores, automation workflows, and personal tools via a context-aware API layer.

## ğŸ’¡ Overview

The system acts as an intelligent assistant that:
- Accepts input from browser, CLI, voice, or API
- Differentiates between **personal** and **work** contexts
- Uses **local LLMs** where possible, **API fallback** where necessary
- Delegates execution (e.g., sending a calendar invite) to **n8n**
- Supports **multi-user mode** (e.g. me + my fiancÃ©e), with scoped tools/memory

## ğŸ“¦ Components

| Service       | Description                                   |
|---------------|-----------------------------------------------|
| `n8n`         | Orchestration and external API execution      |
| `ollama`      | Local model runtime for LLM inference         |
| `chromadb`    | Vector memory (long-term, user/context aware) |
| `agent-api`   | Core orchestrator backend                     |
| `frontend`    | (Optional) UI for chat + voice I/O            |
| `whisper`     | (Optional) Voice transcription service        |

---

## ğŸš€ Phase 0 Goals

âœ… Self-hosted base stack on Proxmox  
âœ… Local LLM running via Ollama  
âœ… ChromaDB running with persistent volume  
âœ… Agent API accepting prompts and returning completions  
âœ… n8n accessible for automations and flow building

---

## ğŸ“‚ Project Structure

mcp-agent-server/ 
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ services/ 
â”‚ â”œâ”€â”€ agent-api/ # Node or Python API backend
â”‚ â”œâ”€â”€ n8n/ # n8n data + extensions
â”‚ â”œâ”€â”€ whisper/ # Voice-to-text logic (optional) 
â”‚ â””â”€â”€ ... 
â”œâ”€â”€ docs/ # Architecture, design, integrations 
â””â”€â”€ .cursor-rules # Cursor IDE custom ruleset

---

## ğŸ” Philosophy

This system avoids unnecessary cloud reliance and emphasizes:

- ğŸ”’ **Privacy**: user data never leaves the server unless explicitly intended
- ğŸ’¡ **Local-first**: local LLMs, self-hosted vector memory, internal service mesh
- ğŸ”§ **Modularity**: everything is Dockerized and composable
- ğŸ§  **Agent-Oriented**: LLMs reason, n8n executes

---

## ğŸ› ï¸ Getting Started

```bash
# Clone and configure
git clone your-repo-url
cd mcp-agent-server
cp .env.example .env
docker-compose up -d
```*